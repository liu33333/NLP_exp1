{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新冠肺炎疫情分析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入\n",
    "- 使用2019.12.23-2020.2.21的微博热搜（top50）数据\n",
    "    - 原排行榜小时级更新，为了便于分析，选取每天12:00的数据作为当天搜索排行榜数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始数据\n",
    "\n",
    "- 微博热搜1223-0210.csv：原始小时级数据\n",
    "   - 一共近8w条 (79278)，每条数据5个字段\n",
    "   - 格式：【时间 排名 热搜内容 最后时间 上榜时间】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa7 in position 14: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# 导入小时级数据\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m df_hour \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m微博热搜标注.xlsx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df_hour\u001B[38;5;241m.\u001B[39mfilter(like\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m钟南山\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#df1 = df_hour.filter(regex='12:00$', axis=0)\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1753\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1750\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1752\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1754\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1755\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:79\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m     kwds\u001B[38;5;241m.\u001B[39mpop(key, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     78\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ensure_dtype_objs(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[43mparsers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munnamed_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39munnamed_cols\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:547\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:636\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._get_header\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1965\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xa7 in position 14: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 导入小时级数据\n",
    "df_hour = pd.read_csv(open(\"微博热搜1223-0210.csv\", encoding='utf8'), sep=',',dtype=str)\n",
    "df1 = df_hour.filter(like='钟南山', axis=0)\n",
    "#df1 = df_hour.filter(regex='12:00$', axis=0)\n",
    "print(df_hour.size, df_hour.shape)\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 疫情相关数据\n",
    "\n",
    "- 原始数据是小时级，量大，而且很多热搜内容与疫情无关，不便分析，需要提炼疫情相关的内容，单独分析\n",
    "- 微博热搜1.1-2.21.csv：缩减后的天级数据，仅保留疫情相关的搜索词，人工标注完成\n",
    "    - 一共899条，数据量大规模减少，8w→900条\n",
    "    - 格式【时间 排名 热搜内容 标注1 标注2 上榜时间 最后时间】\n",
    "    - 注：\n",
    "        - 标注1：地域信息，取值范围：全国数据、疫情地图、地区、新闻、人物、医院、物质、军队、新闻、知识、专家\n",
    "        - 标注2：主题信息，取值范围："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#str_p = '2020/2/7 12:00'\n",
    "#dateTime_p = datetime.datetime.strptime(str_p,'%Y/%m/%d %H:%M')\n",
    "\n",
    "# 导入天级数据，1.30以后标注完整（地域+主题）\n",
    "df_day = pd.read_csv(open(\"微博热搜1.1-2.21.csv\", encoding='gbk'), sep=',',dtype=str)\n",
    "# 时间格式矫正，规范，便于字符串排序\n",
    "df_day['时间'] = df_day['时间'].apply(lambda x:str(datetime.datetime.strptime(x,'%Y/%m/%d %H:%M')).split(' ')[0])#.sort_index()\n",
    "# 按照日期升序排列\n",
    "df_day = df_day.sort_values(by='时间') \n",
    "print(df_day.size, df_day.shape)\n",
    "df_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day.sort_values(by='时间', ascending=False) # 逆序排列\n",
    "df_day[df_day['排名'].isin(['1','2','3'])] # 提取每天的前三名\n",
    "df_day[df_day['时间']=='2020-02-06'].head() # 查看某天的热门搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#df_new.groupby(['时间']).count().sort_values(by='时间')\n",
    "df_day.groupby(['时间'])['排名'].agg([sum])\n",
    "#df_day['时间'].groupby(['时间']).count().reset_index().rename(columns={'0':'数目'})\n",
    "#df_day['排名'].astype(float).groupby(['时间']).size()\n",
    "#df_day['排名'].astype(float).groupby(by=['时间'])\n",
    "df_day.groupby('时间')['排名'].count().reset_index().rename(columns={'0':'数目'})\n",
    "df_day.groupby('时间').count()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 趋势分析\n",
    "- 疫情相关热词近期有什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 疫情热度如何变化？\n",
    "- 每天疫情相关热门搜索词有多少？\n",
    "    - 趋势图见末尾，分析如下：\n",
    "    - 1.19前，只有1-2个热搜，此时疫情刚刚爆发，原因不明，只知道是冠状病毒，不确定是否人传人\n",
    "    - 1.20开始，突然上涨到8个热搜，武汉新增136例，卫健委发布病毒性肺炎高发注意事项，同时疫情已经开始蔓延到省外：浙江发现5例、深圳出现8例（1例确诊）、北京确诊2例，上海也开始加强筛查\n",
    "    - 1.21以后，武汉同济协和医院出现15名医护感染，钟南山站肯定新型冠状病毒肺炎人传人，民众开始高度关注，top 50榜单里至少一半是疫情相关\n",
    "    - 1.23，武汉封城，31个热搜\n",
    "    - 1.25，物资缺乏，除夕夜医护吃泡面，承受极大的压力，接近奔溃，大年初一，政府开始调度资源，医护、军队进入疫区\n",
    "    - 1.26，榜单达到高峰：41个热搜，全国疫情迅速发展，民众焦虑，不知所措\n",
    "    - 1.29，热词数目开始下滑，31个，小汤山医院投入使用，火神山马上完工，虽然各地确诊人数仍然不断增加，但大家没那么害怕\n",
    "    - 2.6，小高峰，35个\n",
    "    - 2.20，减少到15个\n",
    "    - 其他分析。。。\n",
    "- 每天疫情相关热门搜索词平均排名高吗？\n",
    "    - 前期，热词少，突发情况排名较高，如武汉出现疫情\n",
    "    - 中期，平均排名15-20，霸占榜单前几名\n",
    "    - 后期，继续上升到8-10\n",
    "- 每天关注度是如何变化？\n",
    "    - >关注度 = 热门搜索排名累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "day_info_dict = {} # 趋势信息\n",
    "loc_info_dict = {'total':{}, 'overseas':{}, 'prov':{}, 'city':{}} # 地域信息\n",
    "#for i in df.iteritems(): # iterrows()\n",
    "for i in df_day.iterrows():\n",
    "    # 记录每天的搜索词数目\n",
    "    (dt, rank, query, location, topic) = i[1]['时间'], float(i[1]['排名']), i[1]['热搜内容'], i[1]['地域'], i[1]['主题']\n",
    "    if dt not in day_info_dict:\n",
    "        # 记录排名信息，最大，最小，平均\n",
    "        day_info_dict[dt] = {'query_num':1, 'min_rank':50-rank, 'max_rank':50-rank, 'avg_rank':50-rank}\n",
    "    else:\n",
    "        day_info_dict[dt]['query_num'] += 1 # 热词数目\n",
    "        if rank < day_info_dict[dt]['min_rank']:\n",
    "            day_info_dict[dt]['min_rank'] = 50-rank\n",
    "        if rank > day_info_dict[dt]['max_rank']:\n",
    "            day_info_dict[dt]['max_rank'] = 50-rank\n",
    "        day_info_dict[dt]['avg_rank'] = round((day_info_dict[dt]['query_num']+day_info_dict[dt]['avg_rank']*(\n",
    "            day_info_dict[dt]['query_num']-1))/day_info_dict[dt]['query_num'],2)\n",
    "    # 地域信息 国内-湖北-武汉，国外-日本\n",
    "    loc_list = location.split('-')\n",
    "    loc_len = len(loc_list)\n",
    "    if loc_len > 0: # 国内外信息\n",
    "        area = loc_list[0]\n",
    "        if area not in loc_info_dict['total']:\n",
    "            loc_info_dict['total'][area] = [0, 0] # 频次、累计热度（50-名次）\n",
    "        loc_info_dict['total'][area][0] += 1\n",
    "        loc_info_dict['total'][area][1] += 50-rank # 排名越靠前，权重越大\n",
    "    if loc_len > 1: # 省份信息、国家信息\n",
    "        prov = loc_list[1].split('|')[0] # 复合形式：山东|浙江\n",
    "        k = 'prov'\n",
    "        if area == '国外':\n",
    "            k = 'overseas'\n",
    "        if prov not in loc_info_dict[k]:\n",
    "            loc_info_dict[k][prov] = [0, 0, {}] # 频次、累计热度、城市信息\n",
    "        loc_info_dict[k][prov][0] += 1\n",
    "        loc_info_dict[k][prov][1] += 50-rank # 排名越靠前，权重越大\n",
    "    if loc_len > 2: # 城市信息\n",
    "        city =  loc_list[2]\n",
    "        if city not in loc_info_dict[k][prov][2]:\n",
    "            loc_info_dict[k][prov][2][city] = [0,0]\n",
    "        loc_info_dict[k][prov][2][city][0] += 1\n",
    "        loc_info_dict[k][prov][2][city][1] += 50-rank\n",
    "        # 所有城市\n",
    "        if city not in loc_info_dict['city']:\n",
    "            loc_info_dict['city'][city] = [0,0]\n",
    "        loc_info_dict['city'][city][0] += 1\n",
    "        loc_info_dict['city'][city][1] += 50-rank\n",
    "#print(day_info_dict)\n",
    "out_dict = {'趋势':{'日期':[], '热词数目':[], '最大关注度':[], '最小关注度':[], '平均关注度':[]},\n",
    "            '地域':{}}\n",
    "out_dict['趋势']['日期'] = list(day_info_dict.keys())\n",
    "out_dict['趋势']['热词数目'] = [day_info_dict[dt]['query_num'] for dt in out_dict['趋势']['日期']]\n",
    "out_dict['趋势']['最大关注度'] = [day_info_dict[dt]['max_rank'] for dt in out_dict['趋势']['日期']]\n",
    "out_dict['趋势']['最小关注度'] = [day_info_dict[dt]['min_rank'] for dt in out_dict['趋势']['日期']]\n",
    "out_dict['趋势']['平均关注度'] = [day_info_dict[dt]['avg_rank'] for dt in out_dict['趋势']['日期']]\n",
    "#print(json.dumps(out_dict, ensure_ascii=False))\n",
    "tmp_df = pd.DataFrame(out_dict['趋势'])\n",
    "tmp_df[:10]\n",
    "# 世界地图只支持英文，需要先翻译过去\n",
    "country_dict = {'泰国':'Thailand', '日本':'Japan', '美国':'United States', '英国':'United Kingdom', '韩国':'Korea', '新加坡':'Singapore', '法国':'French',\n",
    "                '德国':'German', '巴西':'Brazil', '澳大利亚':'Australian', '芬兰':'Finland', '意大利':'Italian', '俄罗斯':'Russia', \n",
    "                '伊拉克':'Iraq', '伊朗':'Iran', '叙利亚':'Syria', '以色列':'Israel','巴勒斯坦':'Palestine','蒙古':'Mongolia','朝鲜':'Dem. Rep. Korea',\n",
    "                '印度':'Indian', '阿富汗':'Afghanistan', '加拿大':'Canadia', '中国':'China'}\n",
    "out_dict['地域']['总体'] = loc_info_dict['total']\n",
    "out_dict['地域']['世界'] = {\n",
    "    '国家':[country_dict[k] for k,v in loc_info_dict['overseas'].items() if k in country_dict],\n",
    "    '频次':[v[0] for k,v in loc_info_dict['overseas'].items() if k in country_dict],\n",
    "    '关注度':[v[1] for k,v in loc_info_dict['overseas'].items() if k in country_dict],\n",
    "}\n",
    "# 补充中国总体信息\n",
    "out_dict['地域']['世界']['国家'].append('China')\n",
    "out_dict['地域']['世界']['频次'].append(loc_info_dict['total']['国内'][0])\n",
    "out_dict['地域']['世界']['关注度'].append(loc_info_dict['total']['国内'][1])\n",
    "# 省份信息\n",
    "out_dict['地域']['中国'] = {\n",
    "    '省份':[k for k,v in loc_info_dict['prov'].items()],\n",
    "    '频次':[v[0] for k,v in loc_info_dict['prov'].items()],\n",
    "    '关注度':[v[1] for k,v in loc_info_dict['prov'].items()],\n",
    "}\n",
    "# 湖北城市信息\n",
    "out_dict['地域']['湖北'] = {\n",
    "    '城市':[k for k,v in loc_info_dict['prov']['湖北'][2].items()],\n",
    "    '频次':[v[0] for k,v in loc_info_dict['prov']['湖北'][2].items()],\n",
    "    '关注度':[v[1] for k,v in loc_info_dict['prov']['湖北'][2].items()],\n",
    "}\n",
    "# 全国城市信息\n",
    "out_dict['地域']['城市'] = {\n",
    "    '城市':[k for k,v in loc_info_dict['city'].items()],\n",
    "    '频次':[v[0] for k,v in loc_info_dict['city'].items()],\n",
    "    '关注度':[v[1] for k,v in loc_info_dict['city'].items()],\n",
    "}\n",
    "\n",
    "print(loc_info_dict['total'])\n",
    "print(loc_info_dict['prov'])\n",
    "print(loc_info_dict['overseas'])\n",
    "print(loc_info_dict['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主题分析\n",
    "- 热搜内容分布在不同主题里\n",
    "    - 疫情数据：全国确诊人数、疫情地图\n",
    "    - 医护\n",
    "    - 专家\n",
    "    - 政府措施\n",
    "    - 其他\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以全国确诊人数为例，提取“全国累计确诊”前缀的热搜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_c_total = df_day[df_day['热搜内容'].str.contains('全国累计确诊*')] # 字符串匹配，查找类似搜索\n",
    "# 提取热搜里的数字\n",
    "pattern_num = re.compile(r'[\\D+]*(\\d+).*')\n",
    "\n",
    "def get_number(text):\n",
    "    \"\"\" 提取数字 \"\"\"\n",
    "    res = pattern_num.match(text)\n",
    "    if not res:\n",
    "        return 0\n",
    "    return res.groups()[0]\n",
    "total_num_dict = {}\n",
    "# for i in df_c_total.iterrows():\n",
    "#     # 记录每天的搜索词数目\n",
    "#     (dt, rank, query) = i[1]['时间'].split()[0], float(i[1]['排名']), i[1]['热搜内容']\n",
    "df_c_total['确诊人数'] = df_c_total['热搜内容'].apply(get_number)\n",
    "out_dict['确诊'] = {'日期':list(df_c_total['时间']), '排名':list(df_c_total['排名']), '确诊人数':list(df_c_total['确诊人数'])}\n",
    "#out_dict['确诊']\n",
    "df_c_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_map = df_day[df_day['热搜内容'].str.contains('地图')] # 字符串匹配，查找类似搜索\n",
    "#df_c_map\n",
    "out_dict['疫情地图'] = {'日期':list(df_c_map['时间']), '关注度':list(df_c_map['排名'])}\n",
    "df_c_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day[df_day['热搜内容'].str.contains('[医生|护士]')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 疫情内容分析\n",
    "- 采用NLP技术肢解热门搜索词，尝试挖掘内部隐藏的信息\n",
    "    - 参考：[Python的jieba分词及TF-IDF和TextRank 算法提取关键字](https://blog.csdn.net/bozhanggu2239/article/details/80157305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import jieba\n",
    "from jieba import analyse\n",
    "import jieba.posseg as pseg\n",
    "#import matplotlib.pyplot as plt\n",
    "#import jieba.analyse\n",
    "\n",
    "# 分词\n",
    "# sent_words = [list(jieba.cut(sent0)) for sent0 in sentences]\n",
    "# document = [\" \".join(sent0) for sent0 in sent_words]\n",
    "# print(document)\n",
    "# 自定义词库\n",
    "#jieba.set_dictionary(file_name)\n",
    "#jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "entity_list = ['新型冠状病毒/8/nz', '冠状病毒/8/nz','疫情地图/8/nz','武汉肺炎/8/nz','人传人','病毒/8/nz','疫情/8/nz',\n",
    "               '病例','新冠/8/nz', 'SARS/8/nz','隔离', '口罩', '卫健委/5/nt','双黄连/8/nz','埃博拉/3/nz','瑞德西韦/5/nz',\n",
    "           '金银潭/5/nz','百步亭/5/nz','火神山/5/nz','雷神山/5/nz','张家界/5/ns', '小汤山/5/nz','威斯特丹号/5/nz','钻石公主号/5/nz',\n",
    "               '方舱医院/5/nz','同济医院/3/nz','协和医院/3/nz','紫外线', '蝙蝠/3/nz','酒精','消毒','确诊','连闯/2/v',\n",
    "               '康复/3/n','封城/3/n','汤圆/3/n', '武软/3/nt', '红十字会/3/nt', '马拉松/3/n', '小姐姐/3/r',\n",
    "               '医护人员/5/nr','医疗队/5/nr','医院院长/5/nr','张定宇/5/nr','护士/5/nr','医生/5/nr',\n",
    "               '钟南山/5/nr', '李兰娟/5/nr','李文亮/5/nr', '应勇/5/nr','王贺胜/5/nr', '王忠林/5/nr']\n",
    "for w in entity_list:\n",
    "    #print(w)\n",
    "    w_list = w.split('/')\n",
    "    freq, tag = 1, None\n",
    "    if len(w_list) > 1:\n",
    "        # [word freq]\n",
    "        freq = w_list[1]\n",
    "    if len(w_list) > 2:\n",
    "        # [word freq tag]\n",
    "        tag = w_list[2]\n",
    "    #jieba.add_word(w)\n",
    "    jieba.add_word(w_list[0], freq=freq, tag=tag)\n",
    "\n",
    "word_list = []\n",
    "for index, row in df_day.iterrows():\n",
    "    content = row[2]\n",
    "    #TextRank 关键词抽取，只获取固定词性\n",
    "    #words = analyse.textrank(content, topK=50, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "    #words = analyse.textrank(content, topK=50)\n",
    "    # res = analyse.extract_tags(text) # TF-IDF关键词提取\n",
    "    # 不用关键词工具，因为无法导入自定义词库\n",
    "    for w in pseg.cut(content):\n",
    "        if not ( w.flag.startswith('n') or w.flag in ('v','vn','x')):\n",
    "            continue\n",
    "        if len(w.word) == 1:\n",
    "            continue\n",
    "        # 记录全局分词\n",
    "        word_list.append({'word':w.word,'pos':w.flag, 'count':1})\n",
    "df_kw = pd.DataFrame(word_list)\n",
    "# 词频统计\n",
    "df_kw = df_kw.groupby(['word','pos'])['count'].sum().reset_index().sort_values(by='count', ascending=False)\n",
    "df_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day[df_day['热搜内容'].str.contains('连闯')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t'.join(df_kw['word'][:100]))\n",
    "df_day[df_day['热搜内容'].str.contains('连闯')]\n",
    "#df_kw[df_kw['word'].str.contains('小汤山')]\n",
    "# 关键词分门别类\n",
    "tag_dict = {'n':'名词', 'nr':'人物', 'ns':'地名','nz':'专名', 'nt':'机构组织', 'v':'动词', 'vn':'动作'}\n",
    "# 词性：n,v,ns,nr,vn,nt,nz,x,ng,nrt\n",
    "df_kw[df_kw.pos=='nr'][:50] # 人名\n",
    "#df_kw[df_kw.pos=='ns'][:50] # 各地地名\n",
    "#df_kw[df_kw.pos=='n'][:50] # 名词\n",
    "#df_kw[df_kw.pos=='v'][:50] # 动词\n",
    "#df_kw.groupby(['pos']).count().sort_values(by='count', ascending=False)\n",
    "df_kw[df_kw.pos=='vn'][:50] # 动作\n",
    "df_kw[df_kw.pos=='nt'][:50] # 各机构\n",
    "out_dict['关键词'] = {}\n",
    "for tag in tag_dict:\n",
    "    out_dict['关键词'][tag] = {'含义': tag_dict[tag], \n",
    "                '数据':df_kw[['word','count']][df_kw.pos==tag].values.tolist()}\n",
    "#out_dict['关键词']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "#df_kw\n",
    "content = '钟南山承认人传人现象，李文亮说了'\n",
    "words = analyse.textrank(content, topK=50) \n",
    "words = analyse.extract_tags(content, topK=50)\n",
    "print(words)\n",
    "pos = [ '{}/{}'.format(w.word,w.flag) for w in pseg.cut(content) if w.flag.startswith('n') or w.flag in ('v','vn','x')]\n",
    "print('\\t'.join(pos))\n",
    "print('\\t'.join(df_kw[df_kw.pos=='nr']['word']))\n",
    "#df_kw[['word','count']][df_kw.pos=='nr'][:100].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day[df_day['热搜内容'].str.contains('连闯')]\n",
    "df_day.values.tolist()\n",
    "#list(zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国']['关注度']))\n",
    "[ list(a) for a in zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国']['关注度'])][:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先安装pip install pyecharts==0.5.1\n",
    "#from IPython.display import HTML, SVG\n",
    "# from IPython.core.magic import register_cell_magic\n",
    "import pyecharts as pe\n",
    "\n",
    "page = pe.charts.Page('疫情搜索词分析')\n",
    "opts = pe.options # 配置信息\n",
    "t1 = '疫情搜索词分析（每天12:00的数据）'\n",
    "s1 = '2020-2-24'\n",
    "# 图像数据\n",
    "image = pe.components.Image()\n",
    "img_src = (\n",
    "    \"http://inews.gtimg.com/newsapp_match/0/11297270903/0\"\n",
    "    #\"http://pics1.baidu.com/feed/72f082025aafa40fb2156a3decbe23497af0195b.jpeg\"\n",
    ")\n",
    "image.add(\n",
    "    src=img_src,\n",
    "    style_opts={\"width\": \"60%\", \"height\": \"100%\", \"style\": \"margin-top: 20px\"},\n",
    ").set_global_opts(\n",
    "    title_opts=pe.options.ComponentTitleOpts(title=\"武汉加油\", subtitle=s1)\n",
    ")\n",
    "page.add(image)\n",
    "# 表格数据\n",
    "table = pe.components.Table()\n",
    "table.add(list(df_day.columns), df_day.values.tolist()[:20])\n",
    "table.set_global_opts(title_opts=pe.options.ComponentTitleOpts(title=t1+'(表格)', subtitle=s1))\n",
    "page.add(table)\n",
    "# (1) 每天热搜数目排名变化\n",
    "line_trend = pe.charts.Line()\n",
    "line_trend.set_global_opts(title_opts=opts.TitleOpts(title='疫情热搜趋势分析',pos_left='center',pos_top='8%',subtitle=s1),\n",
    "                          # legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "line_trend.add_xaxis(out_dict['趋势']['日期'])\\\n",
    "    .add_yaxis('疫情热搜-数目', out_dict['趋势']['热词数目'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5),)\\\n",
    "    .add_yaxis('疫情热搜-最大关注度（50-rank）', out_dict['趋势']['最大关注度'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\\\n",
    "    .add_yaxis('疫情热搜-最小关注度（50-rank）', out_dict['趋势']['最小关注度'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\\\n",
    "    .add_yaxis('疫情热搜-平均关注度（50-rank）', out_dict['趋势']['平均关注度'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\n",
    "#page.add(line)\n",
    "bar = pe.charts.Bar()\n",
    "bar.set_global_opts(title_opts=opts.TitleOpts(title='疫情热搜趋势分析',pos_left='center',subtitle=s1),\n",
    "                    legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "bar.add_xaxis(out_dict['趋势']['日期'])\\\n",
    "    .add_yaxis('疫情热搜-数目', out_dict['趋势']['热词数目'],\n",
    "               label_opts=opts.LabelOpts(is_show=False)) # 隐藏标签，避免重影\n",
    "bar.overlap(line_trend)\n",
    "#page.add(bar)\n",
    "page.add(line_trend)\n",
    "# 确诊人数及疫情地图分析\n",
    "line = pe.charts.Line()\n",
    "line.set_global_opts(title_opts=opts.TitleOpts(title='疫情热搜-确诊人数变化',pos_left='center',pos_top='8%',subtitle=s1),\n",
    "                    # legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "line.add_xaxis(out_dict['确诊']['日期'])\\\n",
    "    .add_yaxis('确诊人数', out_dict['确诊']['确诊人数'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\n",
    "page.add(line)\n",
    "# 排名分析--确诊人数\n",
    "line = pe.charts.Line()\n",
    "line.set_global_opts(title_opts=opts.TitleOpts(title='疫情热搜关注度分析',pos_left='center',pos_top='8%',subtitle=s1),\n",
    "                    # legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "line.add_xaxis(out_dict['确诊']['日期'])\\\n",
    "    .add_yaxis('疫情热搜-确诊人数', out_dict['确诊']['排名'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\n",
    "# 排名分析--疫情地图\n",
    "line1 = pe.charts.Line()\n",
    "line1.set_global_opts(title_opts=opts.TitleOpts(title='疫情热搜关注度分析',subtitle=s1),\n",
    "                      #legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "line1.add_xaxis(out_dict['疫情地图']['日期'])\\\n",
    "    .add_yaxis('疫情热搜-疫情地图', out_dict['疫情地图']['关注度'], linestyle_opts=opts.LineStyleOpts(width=5, opacity=0.5))\n",
    "line.overlap(line1)\n",
    "page.add(line)\n",
    "# 地图可视化\n",
    "#   世界地图\n",
    "for k in ('频次', '关注度'):\n",
    "    map_p = pe.charts.Map()\n",
    "    num_range = (min(out_dict['地域']['中国'][k]), sorted(out_dict['地域']['中国'][k])[-2])\n",
    "    t = \"(国内频次：{}, 关注度:{}，国外频次：{}，, 关注度:{})\".format(\n",
    "                                out_dict['地域']['总体']['国内'][0], out_dict['地域']['总体']['国内'][1],\n",
    "                                out_dict['地域']['总体']['国外'][0], out_dict['地域']['总体']['国外'][1])\n",
    "    map_p.set_global_opts(title_opts=opts.TitleOpts(\n",
    "                            title=\"新冠肺炎疫情全球分布-{}\".format(k),\n",
    "                            pos_left='center',subtitle='微博热搜榜单-{}'.format(t)), \n",
    "                          legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle',orient='vertical'), # 靠左居中垂直排列\n",
    "                          toolbox_opts=opts.ToolboxOpts(),\n",
    "                          visualmap_opts=opts.VisualMapOpts(min_=num_range[0], max_=num_range[1]))\n",
    "    map_p.add(k, zip(out_dict['地域']['世界']['国家'], out_dict['地域']['世界'][k]), maptype=\"world\")\n",
    "    map_p.set_series_opts(label_opts=opts.LabelOpts(is_show=False)) # 世界地图不显示label\n",
    "    #map_p.add('关注度', zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国']['关注度']), maptype=\"china\")\n",
    "    page.add(map_p)\n",
    "\n",
    "# 中国地图\n",
    "for k in ('频次', '关注度'):\n",
    "    map_p = pe.charts.Map()\n",
    "    num_range = (min(out_dict['地域']['中国'][k]), sorted(out_dict['地域']['中国'][k])[-2])\n",
    "    t = \"(国内频次：{}, 关注度:{}，国外频次：{}，, 关注度:{})\".format(\n",
    "                                out_dict['地域']['总体']['国内'][0], out_dict['地域']['总体']['国内'][1],\n",
    "                                out_dict['地域']['总体']['国外'][0], out_dict['地域']['总体']['国外'][1])\n",
    "    map_p.set_global_opts(title_opts=opts.TitleOpts(\n",
    "                            title=\"新冠肺炎疫情全国分布-{}\".format(k),\n",
    "                            pos_left='center',subtitle='微博热搜榜单-{}'.format(t)), \n",
    "                          legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle'),\n",
    "                          toolbox_opts=opts.ToolboxOpts(),\n",
    "                          visualmap_opts=opts.VisualMapOpts(min_=num_range[0], max_=num_range[1]))\n",
    "    map_p.add(k, zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国'][k]), maptype=\"china\")\n",
    "    #map_p.add('关注度', zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国']['关注度']), maptype=\"china\")\n",
    "    page.add(map_p)\n",
    "    # 热力图\n",
    "    geo = pe.charts.Geo()\n",
    "    geo.add_schema(maptype=\"china\")\n",
    "    geo.add(k, list(zip(out_dict['地域']['中国']['省份'], out_dict['地域']['中国'][k])),type_=pe.globals.ChartType.HEATMAP)\n",
    "    geo.set_series_opts(label_opts=opts.LabelOpts(is_show=False)).set_global_opts(\n",
    "        visualmap_opts=opts.VisualMapOpts(),\n",
    "        title_opts=opts.TitleOpts(title=\"各省份关注度热力图\"),\n",
    "    )\n",
    "    page.add(geo)\n",
    "# 湖北省内城市地图\n",
    "for k in ('频次', '关注度'):\n",
    "    map_hubei = pe.charts.Map()\n",
    "    num_range = (min(out_dict['地域']['湖北'][k]), max(out_dict['地域']['湖北'][k]))\n",
    "    map_hubei.set_global_opts(title_opts=opts.TitleOpts(\n",
    "                                title=\"新冠肺炎疫情（湖北省内）-{}\".format(k),\n",
    "                                pos_left='center',subtitle='微博热搜榜单'), \n",
    "                              legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle'),\n",
    "                          toolbox_opts=opts.ToolboxOpts(),\n",
    "                          visualmap_opts=opts.VisualMapOpts(min_=num_range[0], max_=num_range[1]))\n",
    "    city_name = ['{}市'.format(i) for i in out_dict['地域']['湖北']['城市']]\n",
    "    map_hubei.add(k, zip(city_name, out_dict['地域']['湖北'][k]), maptype='湖北')\n",
    "    #map_hubei.add('关注度', zip(city_name, out_dict['地域']['湖北']['关注度']), maptype='湖北')\n",
    "    page.add(map_hubei)\n",
    "# 全国城市信息 loc_info_dict['prov']\n",
    "for k in ('频次', '关注度'):\n",
    "    # 区块图（全国城市绘制无效）\n",
    "#     map_hubei = pe.charts.Map()\n",
    "#     num_range = (min(out_dict['地域']['城市'][k]), max(out_dict['地域']['城市'][k]))\n",
    "#     map_hubei.set_global_opts(title_opts=opts.TitleOpts(\n",
    "#                                 title=\"新冠肺炎疫情（全国）-{}\".format(k),\n",
    "#                                 pos_left='center',subtitle='微博热搜榜单'), \n",
    "#                               legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle'),\n",
    "#                           toolbox_opts=opts.ToolboxOpts(),\n",
    "#                           visualmap_opts=opts.VisualMapOpts(min_=num_range[0], max_=num_range[1]))\n",
    "#     city_name = ['{}市'.format(i) for i in out_dict['地域']['城市']['城市']]\n",
    "#     map_hubei.add(k, zip(city_name, out_dict['地域']['城市'][k]), maptype='china')\n",
    "#     #map_hubei.add('关注度', zip(city_name, out_dict['地域']['湖北']['关注度']), maptype='湖北')\n",
    "#     page.add(map_hubei)\n",
    "    # 热力图\n",
    "    geo = pe.charts.Geo()\n",
    "    geo.add_schema(maptype=\"china\")\n",
    "    geo.add(k, list(zip(out_dict['地域']['城市']['城市'], out_dict['地域']['城市'][k])),type_=pe.globals.ChartType.HEATMAP)\n",
    "    geo.set_series_opts(label_opts=opts.LabelOpts(is_show=False)).set_global_opts(\n",
    "        visualmap_opts=opts.VisualMapOpts(),\n",
    "        title_opts=opts.TitleOpts(title=\"全国城市热力图-{}\".format(k)),\n",
    "    )\n",
    "    page.add(geo)\n",
    "\n",
    "# (2) 标题关键词词云\n",
    "#-饼图\n",
    "top_n = 30\n",
    "pie = pe.charts.Pie()\n",
    "pie.set_global_opts(title_opts=opts.TitleOpts(title='疫情热词分析-饼图',pos_left='center',subtitle=s1),\n",
    "                    legend_opts=opts.LegendOpts(is_show=False),\n",
    "                    #legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle'),\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "pie.add(\"饼图可视化（Top {}）\".format(top_n), df_kw[['word','count']].values.tolist()[:top_n])\n",
    "page.add(pie)\n",
    "\n",
    "# 关键词词云可视化\n",
    "wc = pe.charts.WordCloud()\n",
    "wc.set_global_opts(title_opts=opts.TitleOpts(title='疫情热词分析-{}'.format('全部关键词'),\n",
    "                                            pos_left='center',subtitle=s1))\n",
    "wc.add(\"疫情热词分析-所有关键词\", df_kw[['word','count']].values.tolist(), shape='circle') # word_size_range=[10, 500]\n",
    "page.add(wc)\n",
    "# 显示各类词云\n",
    "for tag in tag_dict:\n",
    "    wc = pe.charts.WordCloud()\n",
    "    wc.set_global_opts(title_opts=opts.TitleOpts(title='疫情热词分析-{}'.format(out_dict['关键词'][tag]['含义']),\n",
    "                                                 pos_left='center',subtitle=s1),\n",
    "                       legend_opts=opts.LegendOpts(pos_left='left',pos_top='middle'),\n",
    "                      toolbox_opts=opts.ToolboxOpts())\n",
    "    wc.add(out_dict['关键词'][tag]['含义'], \n",
    "           out_dict['关键词'][tag]['数据'], shape='circle') # word_size_range=[10, 500]\n",
    "    page.add(wc)\n",
    "\n",
    "# 统一绘制、落地\n",
    "out_file = 'wqw_sari.html'\n",
    "page.render(out_file)\n",
    "#page.render_notebook()\n",
    "#HTML(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(out_dict['地域']['世界'])\n",
    "value = [95.1, 23.2, 43.3, 66.4, 88.5]\n",
    "attr = [\"China\", \"Canada\", \"Brazil\", \"Russia\", \"United States\"]\n",
    "map0 = pe.charts.Map()\n",
    "map0.add(\"世界地图\", zip(attr, value), maptype=\"world\")\n",
    "#map0.render(path=\"世界地图.html\")\n",
    "#map0.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_hubei = pe.charts.Map()\n",
    "num_range = (min(out_dict['地域']['湖北']['频次']), max(out_dict['地域']['湖北']['频次']))\n",
    "map_hubei.set_global_opts(title_opts=opts.TitleOpts(\n",
    "                        title=\"新冠肺炎疫情（湖北省内）\",\n",
    "                        subtitle='微博热搜榜单'), \n",
    "                      toolbox_opts=opts.ToolboxOpts(),\n",
    "                      visualmap_opts=opts.VisualMapOpts(min_=num_range[0], max_=num_range[1]))\n",
    "city_name = ['{}市'.format(i) for i in out_dict['地域']['湖北']['城市']]\n",
    "print(city_name)\n",
    "print(out_dict['地域']['湖北']['频次'])\n",
    "print(out_dict['地域']['湖北']['关注度'])\n",
    "map_hubei.add('频次', zip(city_name, out_dict['地域']['湖北']['频次']), maptype='湖北')\n",
    "map_hubei.add('关注度', zip(city_name, out_dict['地域']['湖北']['关注度']), maptype='湖北')\n",
    "#map_hubei.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
